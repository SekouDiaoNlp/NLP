{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Ry8HsznEyjux",
    "outputId": "2dd71306-e9a4-4257-d219-ecba9081fa1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Oh7gBuhkSzDJ",
    "outputId": "d38c9017-38e6-4177-ae28-28efa0f2aff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snorkel\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/f4/a0e419793075b737f1e86a1642d676dfdb17859887542afa06915136c231/snorkel-0.9.3-py3-none-any.whl (139kB)\n",
      "\r",
      "\u001b[K     |██▍                             | 10kB 20.4MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 20kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 30kB 6.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 40kB 7.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 51kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 61kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 71kB 5.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 81kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 92kB 7.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 102kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 112kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 122kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 133kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 143kB 6.6MB/s \n",
      "\u001b[?25hCollecting networkx<2.4,>=2.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/08/f20aef11d4c343b557e5de6b9548761811eb16e438cee3d32b1c66c8566b/networkx-2.3.zip (1.7MB)\n",
      "\r",
      "\u001b[K     |▏                               | 10kB 21.8MB/s eta 0:00:01\r",
      "\u001b[K     |▍                               | 20kB 29.0MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 30kB 34.0MB/s eta 0:00:01\r",
      "\u001b[K     |▊                               | 40kB 34.4MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 51kB 35.4MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 61kB 37.3MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 71kB 37.9MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 81kB 38.0MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 92kB 39.1MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 102kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 112kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 122kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 133kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 143kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 153kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 163kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 174kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 184kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 194kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███▊                            | 204kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 215kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 225kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 235kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████▌                           | 245kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 256kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 266kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 276kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 286kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 296kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 307kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 317kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 327kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 337kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 348kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 358kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 368kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 378kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 389kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 399kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████▌                        | 409kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████▊                        | 419kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 430kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 440kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 450kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 460kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 471kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 481kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 491kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 501kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 512kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 522kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 532kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 542kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 552kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 563kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 573kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 583kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 593kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 604kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 614kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 624kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 634kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 645kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 655kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 665kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 675kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 686kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 696kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 706kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 716kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 727kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 737kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 747kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 757kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 768kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 778kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 788kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 798kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 808kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 819kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 829kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 839kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 849kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 860kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 870kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 880kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 890kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 901kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 911kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 921kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 931kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 942kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 952kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 962kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 972kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 983kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 993kB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 1.0MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 1.0MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 1.0MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 1.0MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 1.0MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 1.1MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 1.1MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 1.1MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 1.1MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 1.1MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 1.1MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 1.1MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 1.1MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 1.1MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 1.1MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 1.2MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 1.2MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 1.2MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 1.2MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 1.2MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 1.2MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 1.2MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 1.2MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 1.2MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 1.2MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 1.3MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 1.3MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 1.3MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 1.3MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 1.3MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 1.3MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 1.3MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 1.3MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 1.3MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 1.4MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 1.4MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 1.4MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 1.4MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 1.4MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 1.4MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 1.4MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 1.4MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 1.4MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 1.4MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 1.5MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 1.5MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.5MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 1.5MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 1.5MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▋    | 1.5MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 1.5MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 1.5MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 1.5MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 1.5MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 1.6MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 1.6MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 1.6MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 1.6MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 1.6MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 1.6MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.6MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 1.6MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.6MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 1.6MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 1.7MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 1.7MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 1.7MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.7MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 1.7MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 1.7MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 1.7MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 1.7MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.7MB 39.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.8MB 39.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from snorkel) (1.17.4)\n",
      "Collecting munkres==1.1.2\n",
      "  Downloading https://files.pythonhosted.org/packages/64/97/61ddc63578870e04db6eb1d3bee58ad4e727f682068a7c7405edb8b2cdeb/munkres-1.1.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn<0.22.0,>=0.20.2 in /usr/local/lib/python3.6/dist-packages (from snorkel) (0.21.3)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from snorkel) (1.3.2)\n",
      "Collecting tensorboardX<2.0,>=1.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n",
      "\u001b[K     |████████████████████████████████| 194kB 47.2MB/s \n",
      "\u001b[?25hCollecting tqdm<5.0.0,>=4.33.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/62/6f823501b3bf2bac242bd3c320b592ad1516b3081d82c77c1d813f076856/tqdm-4.39.0-py2.py3-none-any.whl (53kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 8.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas<0.26.0,>=0.25.0 in /usr/local/lib/python3.6/dist-packages (from snorkel) (0.25.3)\n",
      "Collecting torch<1.2.0,>=1.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/60/f685fb2cfb3088736bafbc9bdbb455327bdc8906b606da9c9a81bae1c81e/torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (676.9MB)\n",
      "\u001b[K     |████████████████████████████████| 676.9MB 27kB/s \n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx<2.4,>=2.2->snorkel) (4.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.22.0,>=0.20.2->snorkel) (0.14.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX<2.0,>=1.6->snorkel) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX<2.0,>=1.6->snorkel) (3.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas<0.26.0,>=0.25.0->snorkel) (2.6.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas<0.26.0,>=0.25.0->snorkel) (2018.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX<2.0,>=1.6->snorkel) (41.6.0)\n",
      "Building wheels for collected packages: networkx\n",
      "  Building wheel for networkx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for networkx: filename=networkx-2.3-py2.py3-none-any.whl size=1556408 sha256=c52fb47984779c0dd14d869fd72b5b2a6639a27674e56921a5a3e409ed089605\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/63/64/3699be2a9d0ccdb37c7f16329acf3863fd76eda58c39c737af\n",
      "Successfully built networkx\n",
      "\u001b[31mERROR: torchvision 0.4.2 has requirement torch==1.3.1, but you'll have torch 1.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: networkx, munkres, tensorboardX, tqdm, torch, snorkel\n",
      "  Found existing installation: networkx 2.4\n",
      "    Uninstalling networkx-2.4:\n",
      "      Successfully uninstalled networkx-2.4\n",
      "  Found existing installation: tqdm 4.28.1\n",
      "    Uninstalling tqdm-4.28.1:\n",
      "      Successfully uninstalled tqdm-4.28.1\n",
      "  Found existing installation: torch 1.3.1\n",
      "    Uninstalling torch-1.3.1:\n",
      "      Successfully uninstalled torch-1.3.1\n",
      "Successfully installed munkres-1.1.2 networkx-2.3 snorkel-0.9.3 tensorboardX-1.9 torch-1.1.0 tqdm-4.39.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "tqdm"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm==4.36.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/c1/bc1dba38b48f4ae3c4428aea669c5e27bd5a7642a74c8348451e0bd8ff86/tqdm-4.36.1-py2.py3-none-any.whl (52kB)\n",
      "\r",
      "\u001b[K     |██████▏                         | 10kB 24.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 20kB 4.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 30kB 6.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 40kB 7.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 51kB 4.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 61kB 3.5MB/s \n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "  Found existing installation: tqdm 4.39.0\n",
      "    Uninstalling tqdm-4.39.0:\n",
      "      Successfully uninstalled tqdm-4.39.0\n",
      "Successfully installed tqdm-4.36.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "tqdm"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --quiet snorkel\n",
    "!pip install --quiet tqdm==4.36.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "I3mmXtIJA5Co",
    "outputId": "07362662-e7c3-4f09-9af8-419fd226c8d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fWY9IxRA5Cz"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3UOzNtq7A5C-"
   },
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gf2g3LPGA5DA"
   },
   "outputs": [],
   "source": [
    "def get_text_start_pos(text):\n",
    "    pos = 0\n",
    "    match1 = re.search(r\"(addendum|amendment|change request|change order|agreement|sow|statement of work|work order|task order)\\s+(\\S+\\s+){1,30}(by and between|by and among|between|among) (.+?) and (.+?)\", text)\n",
    "    match2 = re.search(r\"(addendum|amendment|change request|change order|agreement|sow|statement of work|work order|task order)\\s+(\\S+\\s+){1,30}(effective|dated|entered|executed|made) (.+?) and (.+?)\", text)\n",
    "    match3 = re.search(r\"(addendum|amendment|change request|change order|agreement|sow|statement of work|work order|task order)(.+?)(the undersigned)(.+?) and (.+?)\", text)\n",
    "    if match1 and match1.start() < 1000:\n",
    "        pos = match1.start()\n",
    "    elif match2 and match2.start() < 1000:\n",
    "        pos = match2.start()\n",
    "    elif match3 and match3.start() < 1000:\n",
    "        pos = match3.start()\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GW539lEXA5DH"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    #Preprocess                \n",
    "    text = text.replace('\\n',' ').lower()\n",
    "    \n",
    "    #Remove non-alpha characters\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    pos = get_text_start_pos(text)\n",
    "    text = text[pos:]\n",
    "    #Remove articles\n",
    "    #articles = ('a', 'an', 'the')\n",
    "    #text = ' '.join([t for t in text.split() if t not in articles])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QNhBtkaRA5DQ"
   },
   "source": [
    "### Import Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IS2tjVblRNpx",
    "outputId": "6c051269-2184-4bc5-ce88-0e3ed834f6db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/drive/My Drive/cms_word2vec_embedding_300.zip\n",
      "  inflating: cms_word2vec_embedding_300.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip '/content/drive/My Drive/labeled_data_new1.zip'\n",
    "!unzip '/content/drive/My Drive/unlabeled_data_new1.zip'\n",
    "!unzip '/content/drive/My Drive/cms_word2vec_embedding_300.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XvKW5XvUA5Dd",
    "outputId": "663727e3-afc6-4c40-bccc-dbb6fc022632"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1397, 3)\n"
     ]
    }
   ],
   "source": [
    "df_labeled = pd.read_csv('labeled_data_new1.csv') #without removing stop words \n",
    "#df_labeled = pd.read_csv('labeled_data.csv') #with removing stop words\n",
    "df_labeled.head()\n",
    "print(df_labeled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "BhBO6eYiA5Dk",
    "outputId": "01ac14c7-8ded-4d9d-fbfd-b816d1a61361",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSA         463\n",
       "Addendum    314\n",
       "Others      250\n",
       "SOW         236\n",
       "NDA         134\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFEDgyYwA5Dz"
   },
   "outputs": [],
   "source": [
    "#Split labelled data into test and dev sets\n",
    "import numpy as np\n",
    "msk = np.random.rand(len(df_labeled)) < 0.8\n",
    "\n",
    "df_dev = df_labeled[msk]\n",
    "df_test = df_labeled[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IsF1UsgQA5D-",
    "outputId": "acdd672b-f3f1-4b70-e5bc-7469ee139509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260, 3) (1137, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df_test.shape, df_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "U91c5NDnA5EF",
    "outputId": "22891d94-107a-4923-accd-c137925d4ae1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSA         374\n",
       "Addendum    255\n",
       "Others      202\n",
       "SOW         195\n",
       "NDA         111\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fCKRUftJA5EQ",
    "outputId": "75af2da6-7a4d-4b52-8ed4-a260e6f73b19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([59, 89, 23, 48, 41])"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = df_test.label.map({'Addendum': 0, 'MSA': 1, 'SOW': 4, 'NDA': 2, 'Others': 3})\n",
    "y_test = np.array(y_test)\n",
    "np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Y2_de8T0A5Ec",
    "outputId": "2b28e660-17d6-4970-bfe7-387dd05db0b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([255, 374, 111, 202, 195])"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dev = df_dev.label.map({'Addendum': 0, 'MSA': 1, 'SOW': 4, 'NDA': 2, 'Others': 3})\n",
    "y_dev = np.array(y_dev)\n",
    "np.bincount(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "VhJg_XX7A5Eo",
    "outputId": "8f7739c9-9f2a-4f0c-8ef4-a54ef5931c1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "df_test.drop('label', axis=1, inplace=True)\n",
    "df_dev.drop('label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlBQvNZvA5Ey"
   },
   "source": [
    "### Import Unlabeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YzgsO0syA5E2",
    "outputId": "1f8993bf-0513-4808-fab7-5f157849d031"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15472, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unlabeled = pd.read_csv('unlabeled_data_new1.csv') #without removing stop words\n",
    "#df_unlabeled = pd.read_csv('unlabeled_data.csv') #with removing stop words\n",
    "df_unlabeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "HNPeV1zkA5E9",
    "outputId": "8b70b5aa-791e-4297-9e53-afb59b096ddd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15467</th>\n",
       "      <td>D16492.pdf.out.html.txt</td>\n",
       "      <td>ert oep ik ie scope of work corporate mobilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15468</th>\n",
       "      <td>D19966.pdf.out.html.txt</td>\n",
       "      <td>w f bof hl lai b statement of work for gss in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15469</th>\n",
       "      <td>D26253.pdf.out.html.txt</td>\n",
       "      <td>statement of work wolters kluwer united states...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15470</th>\n",
       "      <td>D37991.pdf.out.html.txt</td>\n",
       "      <td>statement of work including all exhibits attac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15471</th>\n",
       "      <td>D01957.pdf.out.html.txt</td>\n",
       "      <td>statement of work no sow services definitions...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      filename                                               text\n",
       "15467  D16492.pdf.out.html.txt   ert oep ik ie scope of work corporate mobilit...\n",
       "15468  D19966.pdf.out.html.txt   w f bof hl lai b statement of work for gss in...\n",
       "15469  D26253.pdf.out.html.txt  statement of work wolters kluwer united states...\n",
       "15470  D37991.pdf.out.html.txt  statement of work including all exhibits attac...\n",
       "15471  D01957.pdf.out.html.txt   statement of work no sow services definitions..."
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unlabeled.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OR3YIFBNA5FE"
   },
   "outputs": [],
   "source": [
    "#Split unlabelled data into train and valid sets\n",
    "import numpy as np\n",
    "msk = np.random.rand(len(df_unlabeled)) < 0.02\n",
    "\n",
    "df_train = df_unlabeled[~msk]\n",
    "df_valid = df_unlabeled[msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7wQgPbkMA5FK",
    "outputId": "9495df65-5667-4c2e-968a-7f3d6d64fbf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15171, 2) (301, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape, df_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53hj2QPcA5FQ"
   },
   "source": [
    "### Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MgId7euaA5FZ",
    "outputId": "fb4d3a60-33ad-4368-9208-3e27403f5f83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.labeling import LabelingFunction\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "ABSTAIN = -1\n",
    "MSA = 1\n",
    "SOW = 4\n",
    "ADDENDUM = 0\n",
    "NDA = 2\n",
    "OTHERS = 3\n",
    "\n",
    "labl_functions = []\n",
    "\n",
    "def keyword_lookup(x, keywords, label):\n",
    "    if any(word in x.text.lower() for word in keywords):\n",
    "        return label\n",
    "    return ABSTAIN\n",
    "\n",
    "#===============MSA=======================\n",
    "@labeling_function()\n",
    "def msa_regex_lookup(x):    \n",
    "    nonmsa_keywords = ['sow', 'statement of work', 'addendum', 'amendment', 'confidentiality agreement', 'disclosure agreement']\n",
    "    match1 = re.search(r\"(agreement agreement|master agreement|master services agreement|this agreement)\\s+(\\S+\\s+){1,30}(by and between|by and among|between|among)(.+?) and (.+?)\", x.text)\n",
    "    match2 = re.search(r\"(agreement agreement|master agreement|master services agreement|this agreement)\\s+(\\S+\\s+){1,30}(effective)(.+?) and (.+?)\", x.text)\n",
    "    match3 = re.search(r\"(agreement agreement|master agreement|master services agreement|this agreement)\\s+(\\S+\\s+){1,30}(the undersigned)(.+?) and (.+?)\", x.text)\n",
    "        \n",
    "    if (match1 and not(any(key in x.text[:match1.end()] for key in nonmsa_keywords))) \\\n",
    "        or (match2 and not(any(key in x.text[:match2.end()] for key in nonmsa_keywords))) \\\n",
    "        or (match3 and not(any(key in x.text[:match3.end()] for key in nonmsa_keywords))):\n",
    "        return MSA\n",
    "    return ABSTAIN\n",
    "\n",
    "labl_functions.append(msa_regex_lookup)\n",
    "\n",
    "msa_keywords = ['indemnified party', 'indemnifying party', 'force majeure', 'intellectual industrial', \n",
    "                'wk service provider', 'intellectual industrial property', 'industrial property right', \n",
    "                'privacy restricted data', 'prior written notice', 'force majeure event', 'subject matter hereof']\n",
    "\n",
    "def make_keyword_lf_msa(keywords, label=MSA):\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords[0]}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, label=label))\n",
    "\n",
    "for key in msa_keywords:\n",
    "    labl_functions.append(make_keyword_lf_msa([key]))\n",
    "    \n",
    " \n",
    "#===============ADDENDUM===================\n",
    "@labeling_function()\n",
    "def addendum_regex_lookup(x):\n",
    "    match1 = re.search(r\"(addendum|amendment|change request|change order)\\s+(\\S+\\s+){1,30}(by and between|by and among|between) (.+?) and (.+?)\", x.text)\n",
    "    match2 = re.search(r\"(addendum|amendment)\\s+(\\S+\\s+){1,30}(schedule a|effective) (.+?) and (.+?)\", x.text)\n",
    "    match3 = re.search(r\"(addendum|amendment) (.+?) (the undersigned) (.+?) and (.+?)\", x.text)\n",
    "    \n",
    "    if (match1 and match1.start() < 1000) or (match2 and match2.start() < 1000) or (match3 and match3.start() < 1000):\n",
    "        return ADDENDUM\n",
    "    return ABSTAIN\n",
    "\n",
    "labl_functions.append(addendum_regex_lookup)\n",
    "\n",
    "addendum_keywords = ['addendum number', 'addendum part', 'amendment part', 'term addendum', 'term amendment', 'addendum made entered',\n",
    "                     'addendum entered', 'duration of addendum', 'purpose of addendum', 'addendum executed', 'subsequent addendum', 'amendment number', \n",
    "                     'amendment date', 'amendment entered', 'amendment made', 'amendment executed',  'amendment effective date', \n",
    "                     'addendum may executed', 'effective date addendum', 'agreement hereby amended', 'service agreement amendment']\n",
    "\n",
    "def make_keyword_lf_addendum(keywords, label=ADDENDUM):\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords[0]}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, label=label),\n",
    "    )\n",
    "\n",
    "for key in addendum_keywords:\n",
    "    labl_functions.append(make_keyword_lf_addendum([key]))\n",
    "\n",
    "#===============SOW===================\n",
    "@labeling_function()\n",
    "def sow_regex_lookup(x):    \n",
    "    nonsow_keywords = ['addendum','amendment']\n",
    "    match1 = re.search(r\"(sow|statement of work|work order|task order)\\s+(\\S+\\s+){1,30}(by and between|by and among|executed by|between|entered into)(.+?) and (.+?)\", x.text)\n",
    "    match2 = re.search(r\"(sow|statement of work|work order|task order)\\s+(\\S+\\s+){1,30}(effective) (.+?) and (.+?)\", x.text)\n",
    "    match3 = re.search(r\"(sow|statement of work|work order|task order)\\s+(\\S+\\s+){1,30}(the undersigned) (.+?) and (.+?)\", x.text)\n",
    "       \n",
    "    if (match1 and match1.start() < 1000 and not(any(key in x.text[:match1.end()] for key in nonsow_keywords)) \\\n",
    "        or (match2 and match2.start() < 1000 and not(any(key not in x.text[:match2.end()] for key in nonsow_keywords))) \\\n",
    "        or match3 and match3.start() < 1000 and not(any(key not in x.text[:match3.end()] for key in nonsow_keywords))):\n",
    "        return SOW\n",
    "    return ABSTAIN\n",
    "\n",
    "labl_functions.append(sow_regex_lookup)\n",
    "\n",
    "sow_keywords = ['sow effective date', 'work sow', 'sow shall', 'sow term', 'service sow', 'defined sow', \n",
    "                'specified sow', 'outlined sow', 'addendum sow', 'client sow', 'sow agreement', \n",
    "                'statement work effective', 'sow end date', 'sow duration']\n",
    "\n",
    "def make_keyword_lf_sow(keywords, label=SOW):\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords[0]}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, label=label),\n",
    "    )\n",
    "\n",
    "for key in sow_keywords:\n",
    "    labl_functions.append(make_keyword_lf_sow([key]))\n",
    "\n",
    "#===============NDA===================\n",
    "@labeling_function()\n",
    "def nda_regex_lookup(x):\n",
    "    nda_keywords = ['mutual confidentiality', 'confidentiality agreement', 'disclosure agreement']\n",
    "    match1 = re.search(r\"(disclosure agreement|confidentiality agreement)\\s+(\\S+\\s+){1,30}(by and between|by and among|between|among)(.+?) and (.+?)\", x.text)\n",
    "    \n",
    "    if match1 and match1.start() < 1000 and any(key in x.text for key in nda_keywords):\n",
    "        return NDA\n",
    "    return ABSTAIN\n",
    "\n",
    "labl_functions.append(nda_regex_lookup)\n",
    "\n",
    "nda_keywords = ['mutual confidentiality', 'affiliated entity', 'agreement negotiation', 'disclosure hereunder', \n",
    "                'mutual confidentiality agreement', 'non confidential basis', 'confidential information agent', \n",
    "                'confidentiality non disclosure', 'party certain confidential information',\n",
    "                'party desire disclose party', 'party wish protect','party furnish']\n",
    "\n",
    "def make_keyword_lf_nda(keywords, label=NDA):\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords[0]}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, label=label),\n",
    "    )\n",
    "\n",
    "for key in nda_keywords:\n",
    "    labl_functions.append(make_keyword_lf_nda([key]))\n",
    "    \n",
    "\n",
    "#===============OTHERS===================\n",
    "@labeling_function()\n",
    "def others_lookup(x):\n",
    "    msa = msa_regex_lookup(x)\n",
    "    sow = sow_regex_lookup(x)\n",
    "    addendum = addendum_regex_lookup(x)\n",
    "    nda = nda_regex_lookup(x)\n",
    "    \n",
    "    if msa == ABSTAIN and sow == ABSTAIN and addendum == ABSTAIN and nda == ABSTAIN:\n",
    "        return OTHERS\n",
    "    return ABSTAIN    \n",
    "    \n",
    "labl_functions.append(others_lookup)\n",
    "\n",
    "other_keywords = ['sir madam letter', 'letter inform', 'engagement letter', 'service order form',\n",
    "                  'change request form', 'signature form', 'agreement service order', 'service component order', \n",
    "                  'term service order', 'component order']\n",
    "\n",
    "def make_keyword_lf_others(keywords, label=OTHERS):\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords[0]}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, label=label),\n",
    "    )\n",
    "\n",
    "for key in other_keywords:\n",
    "    labl_functions.append(make_keyword_lf_others([key]))\n",
    "    \n",
    "@labeling_function()\n",
    "def others_keyword_lookup(x):\n",
    "    if all(word not in x.text for word in list(set(msa_keywords + sow_keywords + nda_keywords + addendum_keywords))):\n",
    "        return OTHERS\n",
    "    return ABSTAIN\n",
    "\n",
    "labl_functions.append(others_keyword_lookup)\n",
    "\n",
    "print(len(labl_functions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KsZiGYXEA5Fw"
   },
   "source": [
    "### Apply Label Functions to Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Qi2FQP6IA5F0",
    "outputId": "39e4b794-6ded-45e6-c3b6-82cd7c25d951"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15171/15171 [02:09<00:00, 117.05it/s]\n",
      "100%|██████████| 1137/1137 [00:12<00:00, 94.37it/s]\n",
      "100%|██████████| 301/301 [00:02<00:00, 130.29it/s]\n"
     ]
    }
   ],
   "source": [
    "#Apply the label functions to the train and valid sets\n",
    "applier = PandasLFApplier(lfs=labl_functions)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_dev = applier.apply(df=df_dev)\n",
    "L_valid = applier.apply(df=df_valid)\n",
    "#L_unlabelled = applier.apply(df=df_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RExLhAToA5GG"
   },
   "outputs": [],
   "source": [
    "#Check the coverage of label functions on train set\n",
    "\n",
    "from snorkel.labeling import LFAnalysis\n",
    "LFAnalysis(L=L_train, lfs=labl_functions).lf_summary().sort_values(by='Coverage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4yKNxeuA5GO"
   },
   "outputs": [],
   "source": [
    "#Check the Coverage and Accuracy of label functions on dev set\n",
    "\n",
    "LFAnalysis(L=L_dev, lfs=labl_functions).lf_summary(y_dev).sort_values(by='Emp. Acc.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RkaYPZu2A5GW"
   },
   "source": [
    "### Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gapVRCHxA5GY"
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import LabelModel\n",
    "label_model = LabelModel(cardinality=5, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, lr=0.001, log_freq=50, seed=123)\n",
    "#label_model.fit(L_unlabelled, n_epochs=500, lr=0.001, log_freq=50, seed=123) #this is for gridsearchCV where train and valid split not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "qSH9JJe8A5Gh",
    "outputId": "1afaa168-b36c-4df9-8ba1-ce96cfec79e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Metrics calculated over data points with non-abstain labels only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Model Accuracy:     83.6%\n"
     ]
    }
   ],
   "source": [
    "label_model_acc = label_model.score(L=L_dev, Y=y_dev)[\"accuracy\"]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "PYAm5P0XA5Gt",
    "outputId": "df1b2a73-27c7-4867-86e9-bc0ed2debf11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.78470509e-02, 4.89630438e-02, 6.98495708e-02, 7.45707181e-01,\n",
       "        6.76331539e-02],\n",
       "       [8.44067062e-02, 6.76091216e-01, 1.58283656e-01, 3.69052465e-05,\n",
       "        8.11815165e-02],\n",
       "       [6.78470509e-02, 4.89630438e-02, 6.98495708e-02, 7.45707181e-01,\n",
       "        6.76331539e-02],\n",
       "       ...,\n",
       "       [3.78049358e-27, 9.99999998e-01, 3.86702493e-21, 9.61056660e-44,\n",
       "        1.81553261e-09],\n",
       "       [1.22401894e-02, 1.53024005e-02, 6.40558863e-03, 1.27805935e-09,\n",
       "        9.66051820e-01],\n",
       "       [5.25931912e-03, 1.75839629e-02, 3.84083561e-03, 4.34887761e-08,\n",
       "        9.73315839e-01]])"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_train = label_model.predict_proba(L_train)\n",
    "probs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "SOoXvR5NA5G3",
    "outputId": "2c414da8-c1c4-4b8e-ef72-a88368bfe56d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.78470509e-02, 4.89630438e-02, 6.98495708e-02, 7.45707181e-01,\n",
       "        6.76331539e-02],\n",
       "       [8.44067062e-02, 6.76091216e-01, 1.58283656e-01, 3.69052465e-05,\n",
       "        8.11815165e-02],\n",
       "       [1.27685866e-04, 9.98903056e-01, 4.48491718e-04, 6.92798289e-15,\n",
       "        5.20766443e-04],\n",
       "       ...,\n",
       "       [7.50650196e-05, 2.94037627e-05, 2.56862790e-05, 1.07155980e-17,\n",
       "        9.99869845e-01],\n",
       "       [6.78470509e-02, 4.89630438e-02, 6.98495708e-02, 7.45707181e-01,\n",
       "        6.76331539e-02],\n",
       "       [1.13068237e-01, 8.75640832e-02, 6.52106213e-02, 6.38024269e-05,\n",
       "        7.34093256e-01]])"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_valid = label_model.predict_proba(L_valid)\n",
    "probs_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhOUSom7A5HB"
   },
   "outputs": [],
   "source": [
    "#this is for gridsearchCV where train and valid split not required\n",
    "#probs_full_train = label_model.predict_proba(L_unlabelled)\n",
    "#probs_full_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJhNVJHBA5HH"
   },
   "source": [
    "### Filter unlaabeled data if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Lvbdl6OLA5HI",
    "outputId": "65d5ebb6-6ad3-4dee-e2d4-d567b19522bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\\n    X=df_unlabeled, y=probs_full_train, L=L_unlabelled\\n)'"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n",
    "    X=df_train, y=probs_train, L=L_train\n",
    ")\n",
    "\n",
    "#this is for gridsearchCV where train and valid split not required\n",
    "\"\"\"df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n",
    "    X=df_unlabeled, y=probs_full_train, L=L_unlabelled\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gbKJV3tZA5HR",
    "outputId": "6a73308e-9bbf-4106-b619-8a95ba3e6e58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15171, 2) (15171, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df_train_filtered.shape, probs_train_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B31UpXzYA5Hb"
   },
   "outputs": [],
   "source": [
    "df_train_filtered = df_train_filtered.text.tolist()\n",
    "y_train = probs_train_filtered\n",
    "df_valid = df_valid.text.tolist()\n",
    "y_valid = probs_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Chrgxu6FA5Hz"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "sp8A4fp4A5H2",
    "outputId": "ea2ad9a5-a52b-4c34-a84b-5a8ef9984a17"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"post_seq_train = t.texts_to_sequences(df_train_filtered)\\npost_seq_valid = t.texts_to_sequences(df_valid)\\nX_train = pad_sequences(post_seq_train, maxlen=max_features, padding='post')\\nX_valid = pad_sequences(post_seq_valid, maxlen=max_features, padding='post')\""
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_features = 150 # cut texts after this number of words\n",
    "\n",
    "# prepare tokenizer\n",
    "t = Tokenizer(num_words=20000)\n",
    "t.fit_on_texts(df_train_filtered)\n",
    "t.fit_on_texts(df_valid)\n",
    "\"\"\"post_seq_train = t.texts_to_sequences(df_train_filtered)\n",
    "post_seq_valid = t.texts_to_sequences(df_valid)\n",
    "X_train = pad_sequences(post_seq_train, maxlen=max_features, padding='post')\n",
    "X_valid = pad_sequences(post_seq_valid, maxlen=max_features, padding='post')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "O_LEKaTXA5H9",
    "outputId": "2a9c156a-2906-47c1-f477-f03d0126f5ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13941, 150) (13941, 5) (327, 150) (327, 5)\n"
     ]
    }
   ],
   "source": [
    "#print(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape)\n",
    "#print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_hYR-Oe-A5IK",
    "outputId": "c7bbaff3-93e7-4291-ca14-884f8870487f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401227"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = t.word_index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qd5HYUtEA5IT"
   },
   "outputs": [],
   "source": [
    "#Import Embeddings and create embedding dict\n",
    "embeddings_index = {}\n",
    "f = open('cms_word2vec_embedding_300.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "oInP6HREA5Ig",
    "outputId": "4cf4c015-8943-4ee9-a264-f679c411377f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embedding_layer = Embedding(num_words,\\n                            EMBEDDING_DIM,\\n                            embeddings_initializer=Constant(embedding_matrix),\\n                            input_length=max_features,\\n                            trainable=False)'"
      ]
     },
     "execution_count": 139,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "#Create embedding layer\n",
    "EMBEDDING_DIM = 300\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "\"\"\"embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=max_features,\n",
    "                            trainable=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmRBWTzsi-wF"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), max_features=100)\n",
    "X_train = vectorizer.fit_transform(df_train_filtered)\n",
    "\n",
    "X_dev = vectorizer.transform(df_dev.text.tolist())\n",
    "X_valid = vectorizer.transform(df_valid)\n",
    "X_test = vectorizer.transform(df_labeled.text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eqCqKu8IA5Iu"
   },
   "outputs": [],
   "source": [
    "def get_keras_early_stopping(patience=10, monitor=\"val_acc\"):\n",
    "    \"\"\"Stops training if monitor value doesn't exceed the current max value after patience num of epochs\"\"\"\n",
    "    return tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=monitor, patience=patience, verbose=1, restore_best_weights=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_nFeqpGzA5I5"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "#from utils import get_keras_early_stopping\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "               \n",
    "model = Sequential()\n",
    "#model.add(Embedding(len(word_index)+1, 300, weights=[embedding_matrix], input_length=max_features, trainable=False))\n",
    "model.add(Embedding(len(word_index)+1, 300, input_length=100))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "4mDNxZicA5JA",
    "outputId": "65592406-70be-4438-fe5d-ba51bd10b7e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "lr = 0.001\n",
    "epochs = 25\n",
    "#decay=lr/epochs\n",
    "adam = Adam(lr=lr)\n",
    "\n",
    "model.compile(adam, 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iBczNFA3AUY5"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"lstm-best-model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=-1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_XG6xkJhA5JG",
    "outputId": "4a974b0b-3ebf-4ea5-87c3-614d12501d55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:421: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 120368400 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15171 samples, validate on 301 samples\n",
      "Epoch 1/25\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15171/15171 [==============================] - 287s 19ms/step - loss: 1.1226 - acc: 0.7011 - val_loss: 0.9284 - val_acc: 0.8140\n",
      "Epoch 2/25\n",
      "15171/15171 [==============================] - 276s 18ms/step - loss: 0.9716 - acc: 0.7957 - val_loss: 0.8911 - val_acc: 0.8272\n",
      "Epoch 3/25\n",
      "15171/15171 [==============================] - 275s 18ms/step - loss: 0.9348 - acc: 0.8109 - val_loss: 0.8601 - val_acc: 0.8306\n",
      "Epoch 4/25\n",
      "15171/15171 [==============================] - 275s 18ms/step - loss: 0.9067 - acc: 0.8298 - val_loss: 0.8629 - val_acc: 0.8206\n",
      "Epoch 5/25\n",
      "15171/15171 [==============================] - 273s 18ms/step - loss: 0.8847 - acc: 0.8434 - val_loss: 0.8454 - val_acc: 0.8538\n",
      "Epoch 6/25\n",
      "15171/15171 [==============================] - 275s 18ms/step - loss: 0.8713 - acc: 0.8505 - val_loss: 0.8480 - val_acc: 0.8405\n",
      "Epoch 7/25\n",
      "15171/15171 [==============================] - 275s 18ms/step - loss: 0.8529 - acc: 0.8644 - val_loss: 0.8405 - val_acc: 0.8704\n",
      "Epoch 8/25\n",
      "15171/15171 [==============================] - 273s 18ms/step - loss: 0.8412 - acc: 0.8701 - val_loss: 0.8351 - val_acc: 0.8571\n",
      "Epoch 9/25\n",
      "15171/15171 [==============================] - 271s 18ms/step - loss: 0.8230 - acc: 0.8831 - val_loss: 0.8312 - val_acc: 0.8738\n",
      "Epoch 10/25\n",
      "15171/15171 [==============================] - 269s 18ms/step - loss: 0.8074 - acc: 0.8945 - val_loss: 0.8294 - val_acc: 0.8704\n",
      "Epoch 11/25\n",
      "15171/15171 [==============================] - 266s 18ms/step - loss: 0.7935 - acc: 0.9030 - val_loss: 0.8253 - val_acc: 0.8804\n",
      "Epoch 12/25\n",
      "15171/15171 [==============================] - 267s 18ms/step - loss: 0.7827 - acc: 0.9092 - val_loss: 0.8291 - val_acc: 0.8671\n",
      "Epoch 13/25\n",
      "15171/15171 [==============================] - 268s 18ms/step - loss: 0.7686 - acc: 0.9202 - val_loss: 0.8472 - val_acc: 0.8904\n",
      "Epoch 14/25\n",
      "15171/15171 [==============================] - 268s 18ms/step - loss: 0.7614 - acc: 0.9240 - val_loss: 0.8307 - val_acc: 0.8704\n",
      "Epoch 15/25\n",
      "15171/15171 [==============================] - 266s 18ms/step - loss: 0.7479 - acc: 0.9370 - val_loss: 0.8331 - val_acc: 0.8870\n",
      "Epoch 16/25\n",
      "15171/15171 [==============================] - 267s 18ms/step - loss: 0.7396 - acc: 0.9389 - val_loss: 0.8102 - val_acc: 0.9003\n",
      "Epoch 17/25\n",
      "15171/15171 [==============================] - 271s 18ms/step - loss: 0.7355 - acc: 0.9427 - val_loss: 0.8343 - val_acc: 0.8704\n",
      "Epoch 18/25\n",
      "15171/15171 [==============================] - 270s 18ms/step - loss: 0.7258 - acc: 0.9492 - val_loss: 0.8378 - val_acc: 0.8937\n",
      "Epoch 19/25\n",
      "15171/15171 [==============================] - 275s 18ms/step - loss: 0.7189 - acc: 0.9552 - val_loss: 0.8500 - val_acc: 0.8771\n",
      "Epoch 20/25\n",
      "15171/15171 [==============================] - 277s 18ms/step - loss: 0.7232 - acc: 0.9518 - val_loss: 0.8247 - val_acc: 0.9003\n",
      "Epoch 21/25\n",
      "15171/15171 [==============================] - 276s 18ms/step - loss: 0.7098 - acc: 0.9614 - val_loss: 0.8226 - val_acc: 0.8870\n",
      "Epoch 22/25\n",
      "15171/15171 [==============================] - 271s 18ms/step - loss: 0.7044 - acc: 0.9632 - val_loss: 0.8253 - val_acc: 0.8904\n",
      "Epoch 23/25\n",
      "15171/15171 [==============================] - 269s 18ms/step - loss: 0.7034 - acc: 0.9643 - val_loss: 0.8397 - val_acc: 0.8870\n",
      "Epoch 24/25\n",
      "15171/15171 [==============================] - 269s 18ms/step - loss: 0.6975 - acc: 0.9670 - val_loss: 0.8336 - val_acc: 0.8970\n",
      "Epoch 25/25\n",
      "15171/15171 [==============================] - 269s 18ms/step - loss: 0.7056 - acc: 0.9611 - val_loss: 0.8219 - val_acc: 0.9037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f54d81c0da0>"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=[X_valid, y_valid],\n",
    "          callbacks=[get_keras_early_stopping(),checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vE52ZR5pMjc1"
   },
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCFP5S7kA5JK"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "#from utils import get_keras_early_stopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def create_model(lr=0.001):\n",
    "    # create model\n",
    "    model = Model()\n",
    "    inputs = Input(shape=(max_features,), dtype='int32')\n",
    "    embed = embedding_layer(inputs)\n",
    "    x = Bidirectional(LSTM(64))(embed)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(5, activation='softmax')(x)\n",
    "    model = Model(inputs = inputs, outputs = x)\n",
    "    \n",
    "    adam = Adam(lr=lr)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(adam, 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAeJxMWsA5JS"
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, batch_size=32, epochs=15, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsjRRNHCA5JX"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the grid search parameters\n",
    "#batch_size = [10, 20, 40, 60]\n",
    "#epochs = [10, 25, 50]\n",
    "lr = [0.001, 0.01, 0.1]\n",
    "param_grid = dict(lr=lr)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=5,  verbose=10)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szA5A_KtA5J3"
   },
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ioR5rY4jA5KA"
   },
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HhByzZEnctj2"
   },
   "outputs": [],
   "source": [
    "# load weights\n",
    "model.load_weights(\"lstm-best-model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-yVwzZXA5KD"
   },
   "outputs": [],
   "source": [
    "\"\"\"text_pred = df_labeled.text.tolist()\n",
    "t.fit_on_texts(text_pred)\n",
    "post_seq_test = t.texts_to_sequences(text_pred)\n",
    "x_test = pad_sequences(post_seq_test, maxlen=max_features, padding='post')\"\"\"\n",
    "probs_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "g-gX9E11A5KJ",
    "outputId": "10088df2-1304-4399-e1f0-d1cfed73bf9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1397"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test = df_labeled.label.map({'Addendum': 0, 'MSA': 1, 'SOW': 4, 'NDA': 2, 'Others': 3})\n",
    "len(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7lWBEyvPA5KW",
    "outputId": "9b4d5be0-81c7-44bb-d643-6d1b63c862ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 78.5%\n"
     ]
    }
   ],
   "source": [
    "from snorkel.analysis import metric_score\n",
    "test_acc = metric_score(golds=labels_test, preds=probs_test.argmax(axis=1), metric=\"accuracy\")\n",
    "print(f\"Test Accuracy: {test_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fu4lhNWQMvcy"
   },
   "outputs": [],
   "source": [
    "!cp lstm-best-model.hdf5 '/content/drive/My Drive/lstm-79.6-90.3-95.1.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13CKL2PeA5Kn"
   },
   "source": [
    "### Save Model Probabilities and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S983sE5JA5Kp"
   },
   "outputs": [],
   "source": [
    "train_pred = df_unlabeled.text.tolist()\n",
    "t.fit_on_texts(train_pred)\n",
    "post_seq_test = t.texts_to_sequences(train_pred)\n",
    "x_train = pad_sequences(post_seq_test, maxlen=max_features, padding='post')\n",
    "probs_train = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wx_yUOlJ0rMQ"
   },
   "outputs": [],
   "source": [
    "x_train = vectorizer.transform(df_unlabeled.text.tolist())\n",
    "probs_train = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bPXQAB9yA5Ks",
    "outputId": "949e2599-bb8e-4a58-da45-ec6c69e7bd27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15472"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vAL5jccE1rmL",
    "outputId": "e78bb48b-4900-4a8b-cc37-c56946380234"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15472/15472 [02:14<00:00, 115.26it/s]\n"
     ]
    }
   ],
   "source": [
    "L_unlabelled = applier.apply(df=df_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "V1kZSAreA5Ky",
    "outputId": "59c8d528-4e56-4182-d29a-d62cc3d09a17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 3, ..., 1, 4, 4])"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train = label_model.predict(L_unlabelled)\n",
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0q3zMSL02mAQ",
    "outputId": "ada4bd34-84f4-4049-a6b3-0ec1ad1a54e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1397"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test = df_labeled.label.map({'Addendum': 0, 'MSA': 1, 'SOW': 4, 'NDA': 2, 'Others': 3})\n",
    "len(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_0at-IzLA5K3",
    "outputId": "0fdd488a-f69f-4e12-b8cd-44e1d2d5354c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15472 1397\n",
      "15472 1397\n"
     ]
    }
   ],
   "source": [
    "print(len(labels_train), len(labels_test))\n",
    "print(len(probs_train), len(probs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTo2L1IaA5K8"
   },
   "outputs": [],
   "source": [
    "probs_latent_train = list(zip(probs_train.tolist(), labels_train.tolist()))\n",
    "probs_latent_test = list(zip(probs_test.tolist(), labels_test.tolist()))\n",
    "#probs_latent_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRtgcN5sA5LD"
   },
   "outputs": [],
   "source": [
    "df_probs_train = pd.DataFrame(probs_latent_train, columns=['probabilities', 'label'])\n",
    "df_probs_test = pd.DataFrame(probs_latent_test, columns=['probabilities', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "mKQWsJ9BA5LG",
    "outputId": "144bcfac-7d08-41fd-928c-e7c8395fbeb7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probabilities</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.888213038444519, 0.011493748985230923, 0.03...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.8944848775863647, 0.016249872744083405, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.29428383708000183, 0.06875303387641907, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.9346663355827332, 0.004241131711751223, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.10538450628519058, 0.10119219869375229, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       probabilities  label\n",
       "0  [0.888213038444519, 0.011493748985230923, 0.03...      0\n",
       "1  [0.8944848775863647, 0.016249872744083405, 0.0...      0\n",
       "2  [0.29428383708000183, 0.06875303387641907, 0.0...      0\n",
       "3  [0.9346663355827332, 0.004241131711751223, 0.0...      0\n",
       "4  [0.10538450628519058, 0.10119219869375229, 0.0...      0"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_probs_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWtYHXFNA5LK"
   },
   "outputs": [],
   "source": [
    "df_probs_train.to_csv('/content/drive/My Drive/lstm_79.6_probabilities_train.csv', index = None)\n",
    "df_probs_test.to_csv('/content/drive/My Drive/lstm_79.6_probabilities_test.csv', index = None)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "TextClassification_Snorkel_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

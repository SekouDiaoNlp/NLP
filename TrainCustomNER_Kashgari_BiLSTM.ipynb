{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KASHGARI - BiLSTM Model\n",
    "## Extract Custom entities from SOW documents \n",
    "### New data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inno/venv_doc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/inno/venv_doc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/inno/venv_doc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/inno/venv_doc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/inno/venv_doc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/inno/venv_doc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/inno/venv_doc/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/inno/venv_doc/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/inno/venv_doc/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/inno/venv_doc/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/inno/venv_doc/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/inno/venv_doc/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0905 12:08:01.889104 140696620533568 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import kashgari\n",
    "from kashgari.tasks.labeling.models import BiLSTM_Model\n",
    "from kashgari.embeddings import BERTEmbedding\n",
    "import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data from text file to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_data_to_list(file):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    sub_tokens = []\n",
    "    sub_tags = []\n",
    "    \n",
    "    fp = open(file, 'r')\n",
    "    for line in fp:\n",
    "        words = line.split()\n",
    "        word_len = len(words)\n",
    "        \n",
    "        if word_len == 0:\n",
    "            tokens.append(sub_tokens)\n",
    "            tags.append(sub_tags)\n",
    "            sub_tags = []\n",
    "            sub_tokens = []\n",
    "        if len(words) == 4:\n",
    "            sub_tags.append(words[3])\n",
    "            sub_tokens.append(words[0])\n",
    "    tokens.append(sub_tokens)\n",
    "    tags.append(sub_tags)\n",
    "    \n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create instance of bert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0905 12:08:13.055193 140696620533568 deprecation.py:506] From /home/inno/venv_doc/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0905 12:08:13.859294 140696620533568 deprecation.py:506] From /home/inno/venv_doc/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0905 12:08:59.121497 140696620533568 bert_embedding.py:126] seq_len: 100\n"
     ]
    }
   ],
   "source": [
    "bert_embed = BERTEmbedding('bert_models/',\n",
    "                           task=kashgari.LABELING,\n",
    "                           sequence_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an object of BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_Model(bert_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text file data to required format - Train & Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = txt_data_to_list('data_new/train.txt')\n",
    "valid_x, valid_y = txt_data_to_list('data_new/valid.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0905 12:08:59.203455 140696620533568 deprecation.py:506] From /home/inno/venv_doc/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0905 12:08:59.212143 140696620533568 deprecation.py:506] From /home/inno/venv_doc/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0905 12:08:59.213238 140696620533568 deprecation.py:506] From /home/inno/venv_doc/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 100, 768), ( 22268928    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 100, 768)     1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 100, 768)     0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 100, 768)     76800       Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 100, 768)     0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 100, 768)     1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 100, 768)     2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 100, 768)     0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 100, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 100, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 100, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 100, 768)     0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 100, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 100, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 100, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 100, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 100, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 100, 768)     0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 100, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 100, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 100, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 100, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 100, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 100, 768)     0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 100, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 100, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output (Concatenate)    (None, 100, 3072)    0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "non_masking_layer (NonMaskingLa (None, 100, 3072)    0           Encoder-Output[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_blstm (Bidirectional)     (None, 100, 256)     3277824     non_masking_layer[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout (Dropout)         (None, 100, 256)     0           layer_blstm[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_time_distributed (TimeDis (None, 100, 14)      3598        layer_dropout[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 100, 14)      0           layer_time_distributed[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 110,684,686\n",
      "Trainable params: 3,281,422\n",
      "Non-trainable params: 107,403,264\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0905 12:09:01.421443 140696620533568 deprecation.py:323] From /home/inno/venv_doc/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 78s 20s/step - loss: 1.1680 - acc: 0.6662 - val_loss: 0.5023 - val_acc: 0.9109\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 64s 16s/step - loss: 0.3761 - acc: 0.9072 - val_loss: 0.2325 - val_acc: 0.9308\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 63s 16s/step - loss: 0.2637 - acc: 0.9253 - val_loss: 0.1493 - val_acc: 0.9382\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 65s 16s/step - loss: 0.2073 - acc: 0.9377 - val_loss: 0.1032 - val_acc: 0.9536\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 68s 17s/step - loss: 0.1700 - acc: 0.9483 - val_loss: 0.1514 - val_acc: 0.9592\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 70s 18s/step - loss: 0.1470 - acc: 0.9560 - val_loss: 0.1994 - val_acc: 0.9624\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 71s 18s/step - loss: 0.1234 - acc: 0.9625 - val_loss: 0.1547 - val_acc: 0.9692\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.1093 - acc: 0.9684 - val_loss: 0.0549 - val_acc: 0.9744\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0958 - acc: 0.9719 - val_loss: 0.0853 - val_acc: 0.9756\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.0888 - acc: 0.9765 - val_loss: 0.1052 - val_acc: 0.9761\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 77s 19s/step - loss: 0.0791 - acc: 0.9797 - val_loss: 0.0775 - val_acc: 0.9788\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 77s 19s/step - loss: 0.0697 - acc: 0.9819 - val_loss: 0.0930 - val_acc: 0.9792\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 78s 20s/step - loss: 0.0643 - acc: 0.9835 - val_loss: 0.0655 - val_acc: 0.9800\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 78s 19s/step - loss: 0.0585 - acc: 0.9845 - val_loss: 0.1503 - val_acc: 0.9800\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 79s 20s/step - loss: 0.0542 - acc: 0.9869 - val_loss: 0.0777 - val_acc: 0.9803\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 80s 20s/step - loss: 0.0500 - acc: 0.9869 - val_loss: 0.0497 - val_acc: 0.9832\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 79s 20s/step - loss: 0.0458 - acc: 0.9883 - val_loss: 0.0411 - val_acc: 0.9805\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 79s 20s/step - loss: 0.0439 - acc: 0.9893 - val_loss: 0.0652 - val_acc: 0.9812\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 79s 20s/step - loss: 0.0400 - acc: 0.9901 - val_loss: 0.0714 - val_acc: 0.9808\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 78s 20s/step - loss: 0.0379 - acc: 0.9903 - val_loss: 0.0402 - val_acc: 0.9808\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 80s 20s/step - loss: 0.0357 - acc: 0.9911 - val_loss: 0.0416 - val_acc: 0.9806\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 80s 20s/step - loss: 0.0327 - acc: 0.9920 - val_loss: 0.0387 - val_acc: 0.9812\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 79s 20s/step - loss: 0.0305 - acc: 0.9925 - val_loss: 0.0494 - val_acc: 0.9815\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 79s 20s/step - loss: 0.0284 - acc: 0.9934 - val_loss: 0.1909 - val_acc: 0.9797\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 76s 19s/step - loss: 0.0272 - acc: 0.9936 - val_loss: 0.0656 - val_acc: 0.9818\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0263 - acc: 0.9939 - val_loss: 0.0998 - val_acc: 0.9815\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 76s 19s/step - loss: 0.0246 - acc: 0.9939 - val_loss: 0.0743 - val_acc: 0.9821\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 76s 19s/step - loss: 0.0233 - acc: 0.9945 - val_loss: 0.1027 - val_acc: 0.9808\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 77s 19s/step - loss: 0.0229 - acc: 0.9942 - val_loss: 0.0788 - val_acc: 0.9818\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 77s 19s/step - loss: 0.0219 - acc: 0.9948 - val_loss: 0.0612 - val_acc: 0.9824\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.0195 - acc: 0.9951 - val_loss: 0.0456 - val_acc: 0.9824\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0199 - acc: 0.9953 - val_loss: 0.0425 - val_acc: 0.9829\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0187 - acc: 0.9959 - val_loss: 0.0522 - val_acc: 0.9823\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0179 - acc: 0.9956 - val_loss: 0.0524 - val_acc: 0.9823\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0173 - acc: 0.9956 - val_loss: 0.1020 - val_acc: 0.9824\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0709 - val_acc: 0.9823\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0165 - acc: 0.9961 - val_loss: 0.0686 - val_acc: 0.9815\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0153 - acc: 0.9969 - val_loss: 0.1408 - val_acc: 0.9821\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0145 - acc: 0.9967 - val_loss: 0.1486 - val_acc: 0.9832\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.0142 - acc: 0.9966 - val_loss: 0.0738 - val_acc: 0.9826\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.0138 - acc: 0.9969 - val_loss: 0.2237 - val_acc: 0.9818\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0133 - acc: 0.9971 - val_loss: 0.0495 - val_acc: 0.9827\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.0130 - acc: 0.9963 - val_loss: 0.0748 - val_acc: 0.9826\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0130 - acc: 0.9969 - val_loss: 0.0653 - val_acc: 0.9830\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0125 - acc: 0.9973 - val_loss: 0.0419 - val_acc: 0.9827\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0109 - acc: 0.9972 - val_loss: 0.0410 - val_acc: 0.9826\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0114 - acc: 0.9977 - val_loss: 0.0744 - val_acc: 0.9821\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0110 - acc: 0.9975 - val_loss: 0.0805 - val_acc: 0.9821\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.0101 - acc: 0.9980 - val_loss: 0.1093 - val_acc: 0.9829\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0098 - acc: 0.9979 - val_loss: 0.0958 - val_acc: 0.9811\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0100 - acc: 0.9975 - val_loss: 0.0597 - val_acc: 0.9820\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0095 - acc: 0.9976 - val_loss: 0.0427 - val_acc: 0.9827\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.0096 - acc: 0.9979 - val_loss: 0.0455 - val_acc: 0.9824\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0099 - acc: 0.9974 - val_loss: 0.1588 - val_acc: 0.9814\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0096 - acc: 0.9982 - val_loss: 0.0672 - val_acc: 0.9827\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0098 - acc: 0.9973 - val_loss: 0.0549 - val_acc: 0.9823\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0093 - acc: 0.9979 - val_loss: 0.0901 - val_acc: 0.9820\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0082 - acc: 0.9981 - val_loss: 0.0424 - val_acc: 0.9832\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 72s 18s/step - loss: 0.0080 - acc: 0.9981 - val_loss: 0.0812 - val_acc: 0.9826\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.0087 - acc: 0.9977 - val_loss: 0.0741 - val_acc: 0.9838\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 76s 19s/step - loss: 0.0081 - acc: 0.9982 - val_loss: 0.1460 - val_acc: 0.9827\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0078 - acc: 0.9981 - val_loss: 0.0806 - val_acc: 0.9821\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0076 - acc: 0.9980 - val_loss: 0.0795 - val_acc: 0.9821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.0075 - acc: 0.9985 - val_loss: 0.0809 - val_acc: 0.9836\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0074 - acc: 0.9980 - val_loss: 0.0825 - val_acc: 0.9827\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0067 - acc: 0.9985 - val_loss: 0.0496 - val_acc: 0.9821\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0067 - acc: 0.9984 - val_loss: 0.0900 - val_acc: 0.9820\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.0072 - acc: 0.9982 - val_loss: 0.0618 - val_acc: 0.9824\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0070 - acc: 0.9985 - val_loss: 0.0714 - val_acc: 0.9824\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.0065 - acc: 0.9986 - val_loss: 0.1419 - val_acc: 0.9821\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0071 - acc: 0.9981 - val_loss: 0.2584 - val_acc: 0.9829\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0073 - acc: 0.9982 - val_loss: 0.1024 - val_acc: 0.9832\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 76s 19s/step - loss: 0.0069 - acc: 0.9981 - val_loss: 0.0512 - val_acc: 0.9821\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0064 - acc: 0.9984 - val_loss: 0.0664 - val_acc: 0.9820\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0063 - acc: 0.9984 - val_loss: 0.0553 - val_acc: 0.9824\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0065 - acc: 0.9983 - val_loss: 0.0515 - val_acc: 0.9830\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0064 - acc: 0.9983 - val_loss: 0.0563 - val_acc: 0.9829\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0056 - acc: 0.9985 - val_loss: 0.0739 - val_acc: 0.9833\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.0057 - acc: 0.9984 - val_loss: 0.0568 - val_acc: 0.9827\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0064 - acc: 0.9985 - val_loss: 0.0480 - val_acc: 0.9824\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0054 - acc: 0.9986 - val_loss: 0.1081 - val_acc: 0.9827\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.0053 - acc: 0.9986 - val_loss: 0.1596 - val_acc: 0.9824\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0055 - acc: 0.9987 - val_loss: 0.0690 - val_acc: 0.9827\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.0060 - acc: 0.9984 - val_loss: 0.1100 - val_acc: 0.9821\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0052 - acc: 0.9990 - val_loss: 0.0814 - val_acc: 0.9824\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0049 - acc: 0.9987 - val_loss: 0.0772 - val_acc: 0.9836\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0052 - acc: 0.9988 - val_loss: 0.0523 - val_acc: 0.9827\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0054 - acc: 0.9987 - val_loss: 0.2803 - val_acc: 0.9817\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0045 - acc: 0.9989 - val_loss: 0.0809 - val_acc: 0.9826\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0051 - acc: 0.9985 - val_loss: 0.1838 - val_acc: 0.9809\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.0663 - val_acc: 0.9823\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0581 - val_acc: 0.9827\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.1054 - val_acc: 0.9823\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0057 - acc: 0.9983 - val_loss: 0.0531 - val_acc: 0.9821\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0607 - val_acc: 0.9823\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0050 - acc: 0.9987 - val_loss: 0.0582 - val_acc: 0.9827\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 74s 19s/step - loss: 0.0051 - acc: 0.9986 - val_loss: 0.0819 - val_acc: 0.9826\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 74s 18s/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0919 - val_acc: 0.9827\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 75s 19s/step - loss: 0.0048 - acc: 0.9987 - val_loss: 0.0961 - val_acc: 0.9826\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 73s 18s/step - loss: 0.0050 - acc: 0.9987 - val_loss: 0.0444 - val_acc: 0.9841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff5386db6a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, valid_x, valid_y, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text file data to required format - Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = txt_data_to_list('data_new/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model's performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "        S     0.6000    0.6667    0.6316        18\n",
      "      FEE     1.0000    1.0000    1.0000        13\n",
      "        F     0.5238    0.5500    0.5366        20\n",
      "       S1     0.8000    0.8421    0.8205        19\n",
      "        M     0.6923    0.6429    0.6667        14\n",
      "       S2     0.6667    0.5714    0.6154         7\n",
      "      ORG     0.0000    0.0000    0.0000         1\n",
      "\n",
      "micro avg     0.6989    0.7065    0.7027        92\n",
      "macro avg     0.6939    0.7065    0.6992        92\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'           precision    recall  f1-score   support\\n\\n        S     0.6000    0.6667    0.6316        18\\n      FEE     1.0000    1.0000    1.0000        13\\n        F     0.5238    0.5500    0.5366        20\\n       S1     0.8000    0.8421    0.8205        19\\n        M     0.6923    0.6429    0.6667        14\\n       S2     0.6667    0.5714    0.6154         7\\n      ORG     0.0000    0.0000    0.0000         1\\n\\nmicro avg     0.6989    0.7065    0.7027        92\\nmacro avg     0.6939    0.7065    0.6992        92\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test user input data by passing as list of lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [['This', 'Statement', 'of', 'Work', '(', '???', 'SOW', '??', ')', ',', 'with', 'an', 'effective', 'date', 'of', 'May', '1', ',', '2017', '(', '???', 'SOW', 'Effective', 'Date', '??', ')', 'is', 'made', 'pursuant', 'to', 'the', 'Staffing', 'Services', 'Agreement', ',', 'dated', 'May', '16', ',', '2008', ',', 'between', 'TEKSystems', ',', 'Inc.', '(', '???', 'Supplier', '??', ')', 'and', 'Wolters', 'Kluwer', 'United', 'States', 'Inc.', '(', '???', 'WKUS', '??', ')', ',', 'and', 'subsequently', 'amended', 'on', 'behalf', 'of', 'its', 'Affiliate', ',', 'Clinical', 'Drug', 'Information', ',', 'LLC', '(', '???', 'CDI', '??', ')', '.'],\n",
    "        ['SOW', 'Project', 'Start', 'Date:', 'October', '1,', '2011'],\n",
    "        ['SOW', 'Project', 'End', 'Date:', 'December', '31,', '2011'],\n",
    "        ['SOW', 'COMMENCEMENT', 'DATE :', '4/1/2017'],['SOW', 'EXPIRATION', 'DATE :', '12/31/2017']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call model's predict function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict_entities(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'labels': [{'end': 18, 'entity': 'S1', 'start': 15, 'value': 'May 1 , 2017'},\n",
      "             {'end': 44,\n",
      "              'entity': 'F',\n",
      "              'start': 42,\n",
      "              'value': 'TEKSystems , Inc.'},\n",
      "             {'end': 55,\n",
      "              'entity': 'S',\n",
      "              'start': 51,\n",
      "              'value': 'Wolters Kluwer United States Inc.'}],\n",
      "  'text': 'This Statement of Work ( ??? SOW ?? ) , with an effective date of '\n",
      "          'May 1 , 2017 ( ??? SOW Effective Date ?? ) is made pursuant to the '\n",
      "          'Staffing Services Agreement , dated May 16 , 2008 , between '\n",
      "          'TEKSystems , Inc. ( ??? Supplier ?? ) and Wolters Kluwer United '\n",
      "          'States Inc. ( ??? WKUS ?? ) , and subsequently amended on behalf of '\n",
      "          'its Affiliate , Clinical Drug Information , LLC ( ??? CDI ?? ) .',\n",
      "  'text_raw': ['This',\n",
      "               'Statement',\n",
      "               'of',\n",
      "               'Work',\n",
      "               '(',\n",
      "               '???',\n",
      "               'SOW',\n",
      "               '??',\n",
      "               ')',\n",
      "               ',',\n",
      "               'with',\n",
      "               'an',\n",
      "               'effective',\n",
      "               'date',\n",
      "               'of',\n",
      "               'May',\n",
      "               '1',\n",
      "               ',',\n",
      "               '2017',\n",
      "               '(',\n",
      "               '???',\n",
      "               'SOW',\n",
      "               'Effective',\n",
      "               'Date',\n",
      "               '??',\n",
      "               ')',\n",
      "               'is',\n",
      "               'made',\n",
      "               'pursuant',\n",
      "               'to',\n",
      "               'the',\n",
      "               'Staffing',\n",
      "               'Services',\n",
      "               'Agreement',\n",
      "               ',',\n",
      "               'dated',\n",
      "               'May',\n",
      "               '16',\n",
      "               ',',\n",
      "               '2008',\n",
      "               ',',\n",
      "               'between',\n",
      "               'TEKSystems',\n",
      "               ',',\n",
      "               'Inc.',\n",
      "               '(',\n",
      "               '???',\n",
      "               'Supplier',\n",
      "               '??',\n",
      "               ')',\n",
      "               'and',\n",
      "               'Wolters',\n",
      "               'Kluwer',\n",
      "               'United',\n",
      "               'States',\n",
      "               'Inc.',\n",
      "               '(',\n",
      "               '???',\n",
      "               'WKUS',\n",
      "               '??',\n",
      "               ')',\n",
      "               ',',\n",
      "               'and',\n",
      "               'subsequently',\n",
      "               'amended',\n",
      "               'on',\n",
      "               'behalf',\n",
      "               'of',\n",
      "               'its',\n",
      "               'Affiliate',\n",
      "               ',',\n",
      "               'Clinical',\n",
      "               'Drug',\n",
      "               'Information',\n",
      "               ',',\n",
      "               'LLC',\n",
      "               '(',\n",
      "               '???',\n",
      "               'CDI',\n",
      "               '??',\n",
      "               ')',\n",
      "               '.']},\n",
      " {'labels': [{'end': 6, 'entity': 'S1', 'start': 5, 'value': '1, 2011'}],\n",
      "  'text': 'SOW Project Start Date: October 1, 2011',\n",
      "  'text_raw': ['SOW', 'Project', 'Start', 'Date:', 'October', '1,', '2011']},\n",
      " {'labels': [{'end': 6,\n",
      "              'entity': 'S2',\n",
      "              'start': 4,\n",
      "              'value': 'December 31, 2011'}],\n",
      "  'text': 'SOW Project End Date: December 31, 2011',\n",
      "  'text_raw': ['SOW', 'Project', 'End', 'Date:', 'December', '31,', '2011']},\n",
      " {'labels': [{'end': 3, 'entity': 'F', 'start': 2, 'value': 'DATE : 4/1/2017'}],\n",
      "  'text': 'SOW COMMENCEMENT DATE : 4/1/2017',\n",
      "  'text_raw': ['SOW', 'COMMENCEMENT', 'DATE :', '4/1/2017']},\n",
      " {'labels': [{'end': 3,\n",
      "              'entity': 'F',\n",
      "              'start': 2,\n",
      "              'value': 'DATE : 12/31/2017'}],\n",
      "  'text': 'SOW EXPIRATION DATE : 12/31/2017',\n",
      "  'text_raw': ['SOW', 'EXPIRATION', 'DATE :', '12/31/2017']}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
